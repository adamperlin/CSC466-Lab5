{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ef157162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "import os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "70920515",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_PATH = './stopwords'\n",
    "# replace all non-whitespace, digits, and other non-alphabetic characters\n",
    "to_replace = re.compile(r'(?!\\s)(\\W|\\d+)')\n",
    "# split on any whitespace\n",
    "whitespace_delimiters = re.compile('\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "2596b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple utility function to read stopwords from a list of stopwords files, \n",
    "# and combine them into one big set\n",
    "def read_stopwords(files):\n",
    "    stopwords = set()\n",
    "    for words_file in files:\n",
    "        with open(words_file) as f:\n",
    "            for line in f.readlines():\n",
    "                # strip out all the characters we don't want\n",
    "                line = re.sub(to_replace, '', line)\n",
    "                word = line.strip()\n",
    "                if len(word) > 0:\n",
    "                    stopwords.add(word.lower())\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9513a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines our set of stopwords to throw out\n",
    "STOPWORDS = read_stopwords([os.path.join(STOPWORDS_PATH, p) for p in os.listdir(STOPWORDS_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "389542fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a chunk of text, returns contiguous blocks of alphabetic characters (presumably, words)\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # strip off all the undesirable bits (punctuation, numbers, etc.)\n",
    "    stripped = re.sub(to_replace, '', text)\n",
    "    freqs = defaultdict(lambda: 0)\n",
    "    for word in re.split(whitespace_delimiters, stripped):\n",
    "        if word == '':\n",
    "            continue\n",
    "            \n",
    "        # normalize all words to lowercase\n",
    "        word = word.lower()\n",
    "        # add stemmed word to frequency count if it is not a stopword\n",
    "        if word not in STOPWORDS:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            freqs[stemmed] += 1\n",
    "            \n",
    "        \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f3a9bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the term frequencies for a single document (f_j), all document frequencies (d),\n",
    "# the maximum frequencies for every keyword, and the positions word weights should be placed in,\n",
    "# and the total number of documents,\n",
    "# creates a vector which is the size of the vocabulary, and calculates the tf-idf weights\n",
    "# for this particular set of frequencies.\n",
    "def vectorize_document(freqs, doc_freqs, max_freqs, positions, n):\n",
    "    vec = np.zeros(len(positions))\n",
    "    for keyword in freqs:\n",
    "        tf_idf = (freqs[keyword] / max_freqs[keyword]) * math.log2(n/doc_freqs[keyword])\n",
    "        vec[positions[keyword]] = tf_idf\n",
    "    \n",
    "    return vec\n",
    "\n",
    "# given doc_freqs, a dictionary of terms to the number of documents in which the terms occur,\n",
    "# and name_to_freqs, a dict of dicts mapping of training file names to their respective word frequencies\n",
    "# computes - for every keyword in doc_freqs - the maximum frequency of the word across all documents \n",
    "# we need this for normalizing the keyword frequencies when we vectorize\n",
    "def max_term_frequencies(doc_freqs, name_to_freqs):\n",
    "    max_freqs = {}\n",
    "    for keyword in doc_freqs:\n",
    "            # determine which document has the maximum frequency for a particular keyword\n",
    "            max_freq_doc = max(name_to_freqs, \n",
    "                                    key=lambda doc_name: 0 if keyword not in name_to_freqs[doc_name] else \n",
    "                                        name_to_freqs[doc_name][keyword])\n",
    "            max_freqs[keyword] = name_to_freqs[max_freq_doc][keyword]\n",
    "    \n",
    "    return max_freqs\n",
    "            \n",
    "def extract_words(data_file: str):\n",
    "    with open(data_file, 'r') as f:\n",
    "        return tokenize(f.read())\n",
    "\n",
    "def vectorize_dataset(train_directory: str):\n",
    "    # data file -> author mapping so we can create a ground truth file\n",
    "    authors = {}\n",
    "    # maps training file name -> document frequencies\n",
    "    # essentially a dict of dict(keyword -> keyword count)\n",
    "    name_to_freqs = {}\n",
    "    doc_freqs = defaultdict(lambda: 0)\n",
    "    total_documents = 0\n",
    "    \n",
    "    for author_dir in os.listdir(train_directory):\n",
    "        name = author_dir\n",
    "        fullpath = os.path.join(train_directory, author_dir)\n",
    "        for data_file in os.listdir(fullpath):\n",
    "            authors[data_file] = author_dir\n",
    "            \n",
    "            # determine the term frequency for all words in this data file\n",
    "            freqs = extract_words(os.path.join(fullpath, data_file))\n",
    "            \n",
    "            # ensure that the total document frequency is incremented by one\n",
    "            # for each of the words we found in the document\n",
    "            for word in freqs:\n",
    "                doc_freqs[word] += 1\n",
    "            \n",
    "            # store the word frequencies for this particular datafile\n",
    "            name_to_freqs[data_file] = freqs\n",
    "            \n",
    "            total_documents += 1\n",
    "    \n",
    "    # create ground truth df\n",
    "    ground_truth = pd.DataFrame(index=sorted(d.keys()))\n",
    "    for (k, v) in authors.items():\n",
    "        ground_truth.at[k, 'Author'] = v\n",
    "    \n",
    "    # compute the maximum frequencies for every term\n",
    "    max_freqs = max_term_frequencies(doc_freqs, name_to_freqs)\n",
    "    \n",
    "    # we are creating a vector which is the length of our vocabulary set,\n",
    "    # so we must assign each word a unique 'dimension' in this vector\n",
    "    positions = dict(zip(sorted(doc_freqs), range(len(doc_freqs))))\n",
    "    \n",
    "    rows = []\n",
    "    for train_file in name_to_freqs:\n",
    "        v = vectorize_document(\n",
    "            name_to_freqs[train_file], \n",
    "            doc_freqs, \n",
    "            max_freqs, \n",
    "            positions, \n",
    "            total_documents)\n",
    "        rows.append(v)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=name_to_freqs.keys(),columns=list(range(len(doc_freqs))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4e0beb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = vectorize_dataset('./C50/C50train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ff4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
