{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ef157162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "import os.path\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b6ae09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS_PATH = './stopwords'\n",
    "# replace all non-whitespace, digits, and other non-alphabetic characters\n",
    "to_replace = re.compile(r'(?!\\s)(\\W|\\d+)')\n",
    "# split on any whitespace\n",
    "whitespace_delimiters = re.compile('\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cda96fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple utility function to read stopwords from a list of stopwords files, \n",
    "# and combine them into one big set\n",
    "def read_stopwords(files):\n",
    "    stopwords = set()\n",
    "    for words_file in files:\n",
    "        with open(words_file) as f:\n",
    "            for line in f.readlines():\n",
    "                # strip out all the characters we don't want\n",
    "                line = re.sub(to_replace, '', line)\n",
    "                word = line.strip()\n",
    "                if len(word) > 0:\n",
    "                    stopwords.add(word.lower())\n",
    "    \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "ad74d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines our set of stopwords to throw out\n",
    "STOPWORDS = read_stopwords([os.path.join(STOPWORDS_PATH, p) for p in os.listdir(STOPWORDS_PATH)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "eee6bd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a chunk of text, returns contiguous blocks of alphabetic characters (presumably, words)\n",
    "def tokenize(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # strip off all the undesirable bits (punctuation, numbers, etc.)\n",
    "    stripped = re.sub(to_replace, '', text)\n",
    "    freqs = defaultdict(lambda: 0)\n",
    "    for word in re.split(whitespace_delimiters, stripped):\n",
    "        if word == '':\n",
    "            continue\n",
    "            \n",
    "        # normalize all words to lowercase\n",
    "        word = word.lower()\n",
    "        # add stemmed word to frequency count if it is not a stopword\n",
    "        if word not in STOPWORDS:\n",
    "            stemmed = stemmer.stem(word)\n",
    "            freqs[stemmed] += 1\n",
    "            \n",
    "        \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "f3a9bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the term frequencies for a single document (f_j), all document frequencies (d),\n",
    "# the maximum frequencies for every keyword, and the positions word weights should be placed in,\n",
    "# and the total number of documents,\n",
    "# creates a vector which is the size of the vocabulary, and calculates the tf-idf weights\n",
    "# for this particular set of frequencies.\n",
    "def vectorize_document(freqs, doc_freqs, max_freqs, positions, n):\n",
    "    vec = np.zeros(len(positions))\n",
    "    for keyword in freqs:\n",
    "        tf_idf = (freqs[keyword] / max_freqs[keyword]) * math.log2(n/doc_freqs[keyword])\n",
    "        vec[positions[keyword]] = tf_idf\n",
    "    \n",
    "    return vec\n",
    "\n",
    "# given doc_freqs, a dictionary of terms to the number of documents in which the terms occur,\n",
    "# and name_to_freqs, a dict of dicts mapping of training file names to their respective word frequencies\n",
    "# computes - for every keyword in doc_freqs - the maximum frequency of the word across all documents \n",
    "# we need this for normalizing the keyword frequencies when we vectorize\n",
    "def max_term_frequencies(doc_freqs, name_to_freqs):\n",
    "    max_freqs = {}\n",
    "    for keyword in doc_freqs:\n",
    "            # determine which document has the maximum frequency for a particular keyword\n",
    "            max_freq_doc = max(name_to_freqs, \n",
    "                                    key=lambda doc_name: 0 if keyword not in name_to_freqs[doc_name] else \n",
    "                                        name_to_freqs[doc_name][keyword])\n",
    "            max_freqs[keyword] = name_to_freqs[max_freq_doc][keyword]\n",
    "    \n",
    "    return max_freqs\n",
    "            \n",
    "def extract_words(data_file: str):\n",
    "    with open(data_file, 'r') as f:\n",
    "        return tokenize(f.read())\n",
    "\n",
    "def vectorize_dataset(train_directory: str):\n",
    "    # data file -> author mapping so we can create a ground truth file\n",
    "    authors = {}\n",
    "    # maps training file name -> document frequencies\n",
    "    # essentially a dict of dict(keyword -> keyword count)\n",
    "    name_to_freqs = {}\n",
    "    doc_freqs = defaultdict(lambda: 0)\n",
    "    total_documents = 0\n",
    "    \n",
    "    for author_dir in os.listdir(train_directory):\n",
    "        name = author_dir\n",
    "        fullpath = os.path.join(train_directory, author_dir)\n",
    "        for data_file in os.listdir(fullpath):\n",
    "            authors[data_file] = author_dir\n",
    "            \n",
    "            # determine the term frequency for all words in this data file\n",
    "            freqs = extract_words(os.path.join(fullpath, data_file))\n",
    "            \n",
    "            # ensure that the total document frequency is incremented by one\n",
    "            # for each of the words we found in the document\n",
    "            for word in freqs:\n",
    "                doc_freqs[word] += 1\n",
    "            \n",
    "            # store the word frequencies for this particular datafile\n",
    "            name_to_freqs[data_file] = freqs\n",
    "            \n",
    "            total_documents += 1\n",
    "    \n",
    "    # create ground truth df\n",
    "    ground_truth = pd.DataFrame(index=sorted(d.keys()))\n",
    "    for (k, v) in authors.items():\n",
    "        ground_truth.at[k, 'Author'] = v\n",
    "    \n",
    "    # compute the maximum frequencies for every term\n",
    "    max_freqs = max_term_frequencies(doc_freqs, name_to_freqs)\n",
    "    \n",
    "    # we are creating a vector which is the length of our vocabulary set,\n",
    "    # so we must assign each word a unique 'dimension' in this vector\n",
    "    positions = dict(zip(sorted(doc_freqs), range(len(doc_freqs))))\n",
    "    \n",
    "    rows = []\n",
    "    for train_file in name_to_freqs:\n",
    "        v = vectorize_document(\n",
    "            name_to_freqs[train_file], \n",
    "            doc_freqs, \n",
    "            max_freqs, \n",
    "            positions, \n",
    "            total_documents)\n",
    "        rows.append(v)\n",
    "    \n",
    "    df = pd.DataFrame(rows, index=name_to_freqs.keys(),columns=list(range(len(doc_freqs))))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "c7a95c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = vectorize_dataset('./C50/C50train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "93978315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21712</th>\n",
       "      <th>21713</th>\n",
       "      <th>21714</th>\n",
       "      <th>21715</th>\n",
       "      <th>21716</th>\n",
       "      <th>21717</th>\n",
       "      <th>21718</th>\n",
       "      <th>21719</th>\n",
       "      <th>21720</th>\n",
       "      <th>21721</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147604newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196812newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219316newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251225newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177958newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224725newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233590newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207278newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236474newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188535newsML.txt</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2500 rows × 21722 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1      2      3      4      5      6      7      \\\n",
       "147604newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "196812newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "219316newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "251225newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "177958newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                 ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "224725newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "233590newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "207278newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "236474newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "188535newsML.txt    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                  8      9      ...  21712  21713  21714  21715  21716  21717  \\\n",
       "147604newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "196812newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "219316newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "251225newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "177958newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "...                 ...    ...  ...    ...    ...    ...    ...    ...    ...   \n",
       "224725newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "233590newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "207278newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "236474newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "188535newsML.txt    0.0    0.0  ...    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                  21718  21719  21720  21721  \n",
       "147604newsML.txt    0.0    0.0    0.0    0.0  \n",
       "196812newsML.txt    0.0    0.0    0.0    0.0  \n",
       "219316newsML.txt    0.0    0.0    0.0    0.0  \n",
       "251225newsML.txt    0.0    0.0    0.0    0.0  \n",
       "177958newsML.txt    0.0    0.0    0.0    0.0  \n",
       "...                 ...    ...    ...    ...  \n",
       "224725newsML.txt    0.0    0.0    0.0    0.0  \n",
       "233590newsML.txt    0.0    0.0    0.0    0.0  \n",
       "207278newsML.txt    0.0    0.0    0.0    0.0  \n",
       "236474newsML.txt    0.0    0.0    0.0    0.0  \n",
       "188535newsML.txt    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[2500 rows x 21722 columns]"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532cf1e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
